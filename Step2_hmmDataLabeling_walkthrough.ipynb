{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.ndimage\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition \n",
    "from characterDefinitions import getHandwritingCharacterDefinitions\n",
    "#il notebook usa Hidden Markov Models per etichettare tutti i dataset e \n",
    "#le due partizioni (HeldOutBlocks e HeldOutTrials).\n",
    "#Un Modello di Markov descrive un processo come una serie di stati \n",
    "#con transizioni fra di essi. Ogni transizione ha una probabilità. \n",
    "#In un Hidden Markov Model (HMM), ogni stato ha anche un insieme di possibili output,\n",
    "# ognuno con una sua probabilità.\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = os.path.expanduser('~') + '/handwritingBCIData/'\n",
    "#define which dataset to use as an example (and which sentence to use from that dataset)\n",
    "dataDir = 't5.2019.05.08'\n",
    "sentenceIdx = 15\n",
    "\n",
    "dat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/sentences.mat')\n",
    "singleLetterDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/singleLetters.mat')\n",
    "twCubes = scipy.io.loadmat(rootDir+'LSTMTrainingSteps/Step1_TimeWarping/'+dataDir+'_warpedCubes.mat')\n",
    "\n",
    "#defines the list of all 31 characters and what to call them\n",
    "charDef = getHandwritingCharacterDefinitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence:\n",
      "so>far,>i>haven't>had>a>lot>of>luck>with>that~\n",
      "Example sentence:\n",
      "so>far,>i>haven't>had>a>lot>of>luck>with>that~\n"
     ]
    }
   ],
   "source": [
    "#Take a look at the example sentence (note that '>' denotes a space and '~' a period).\n",
    "sentence = dat['sentencePrompt'][sentenceIdx,0][0]\n",
    "\n",
    "print('Example sentence:')\n",
    "print(sentence)#Take a look at the example sentence (note that '>' denotes a space and '~' a period).\n",
    "sentence = dat['sentencePrompt'][sentenceIdx,0][0]\n",
    "\n",
    "print('Example sentence:')\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/l88ylpp179zc1m2j9r3j1n6w0000gn/T/ipykernel_63566/3998082400.py:10: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  neuralCube = scipy.ndimage.filters.gaussian_filter1d(neuralCube, 4.0, axis=0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m neuralCube \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mndimage\u001b[39m.\u001b[39mfilters\u001b[39m.\u001b[39mgaussian_filter1d(neuralCube, \u001b[39m4.0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#Select the time window of neural activity to use for this template (time step 50 is the 'go' cue)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m neuralCube \u001b[39m=\u001b[39m neuralCube[\u001b[39m59\u001b[39;49m:(\u001b[39m59\u001b[39;49m\u001b[39m+\u001b[39;49mthisCharLen\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m),:]\n\u001b[1;32m     15\u001b[0m \u001b[39m#Use PCA to denoise this template by keeping only the top 10 dimensions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pcaModel \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mdecomposition\u001b[39m.\u001b[39mPCA(n_components\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "#First, we make spatiotemporal 'templates' of neural activity for each character. \n",
    "#The templates will be used to initialize the HMM parameters.\n",
    "\n",
    "#Make a template for each character and store it in the 'templates' dictionary\n",
    "templates = {}\n",
    "for char, charAbbr, thisCharLen in zip(charDef['charList'], charDef['charListAbbr'], charDef['charLen']):\n",
    "    #Average the time-warped data across trials and smooth it\n",
    "    neuralCube = twCubes[char].copy()\n",
    "    neuralCube = np.nanmean(neuralCube, axis=0)\n",
    "    neuralCube = scipy.ndimage.filters.gaussian_filter1d(neuralCube, 4.0, axis=0)\n",
    "    \n",
    "    #Select the time window of neural activity to use for this template (time step 50 is the 'go' cue)\n",
    "    neuralCube = neuralCube[59:(59+thisCharLen+1),:]\n",
    "    \n",
    "    #Use PCA to denoise this template by keeping only the top 10 dimensions\n",
    "    pcaModel = sklearn.decomposition.PCA(n_components=10)\n",
    "    pcaModel.fit(neuralCube)\n",
    "    lowRankTemplate = pcaModel.inverse_transform(pcaModel.transform(neuralCube))\n",
    "    \n",
    "    templates[charAbbr] = lowRankTemplate\n",
    "#Visualize the template activity patterns for a few example characters.\n",
    "#Each pixel (i,j) in the images denotes the mean firing rate observed on electrode i during time step j.\n",
    "exampleChars = ['m','k','s']\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "for char, plotIdx in zip(exampleChars, range(len(exampleChars))):\n",
    "    plt.subplot(1,3,plotIdx+1)\n",
    "    \n",
    "    #Equalize the size of each display template so that they display with the same aspect ratios.\n",
    "    tmp = np.zeros([192, 140])\n",
    "    tmp[:] = np.nan\n",
    "    tmp[:,0:templates[char].shape[0]] = np.transpose(templates[char])\n",
    "\n",
    "    plt.imshow(tmp,clim=[-0.4, 0.6],aspect='auto')\n",
    "    \n",
    "    plt.title(char + ' template')\n",
    "    plt.xlabel('Time Step (10 ms bins)')\n",
    "    plt.ylabel('Electrode #')\n",
    "    \n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'s'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(sentence)):\n\u001b[1;32m     17\u001b[0m     letterStartIdx[x] \u001b[39m=\u001b[39m nStates\n\u001b[0;32m---> 18\u001b[0m     nStates \u001b[39m=\u001b[39m nStates \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mfloor(templates[sentence[x]]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mhmmBinSize) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m#+1 for blank state at the end of each character\u001b[39;00m\n\u001b[1;32m     20\u001b[0m nStates \u001b[39m=\u001b[39m (nStates \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32) \u001b[39m#+1 for blank state at the beginning of the sentence\u001b[39;00m\n\u001b[1;32m     22\u001b[0m letterStartIdx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m#adjust letterStartIdx to account for the blank state at the beginning of the sentence\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 's'"
     ]
    }
   ],
   "source": [
    "#Now we construct the HMM state transitions & emission probabilities for our example sentence.\n",
    "#Each HMM state corresponds to a piece of a character in the sentence.\n",
    "\n",
    "#To speed things up and conserve memory, the HMM runs on larger time bins of data than the 'raw' 10 ms time steps. \n",
    "#hmmBinSize defines the number of 10 ms bins in a single HMM time step\n",
    "hmmBinSize = 5 \n",
    "\n",
    "#The following parameters define state transition probabilities\n",
    "blankProb = 0.1 #probability of entering a blank state at the end of a character\n",
    "stayProb = 0.20 #probability of persisting in the same state\n",
    "skipProb = 0.20 #probability of skipping two states ahead (instead of simply advancing to the next one)\n",
    "\n",
    "#First, compute the total number of states in the HMM (nStates) and the state at which each letter begins (letterStartIdx)\n",
    "nStates = 0\n",
    "letterStartIdx = np.zeros(len(sentence))\n",
    "for x in range(len(sentence)):\n",
    "    letterStartIdx[x] = nStates\n",
    "    nStates = nStates + np.floor(templates[sentence[x]].shape[0]/hmmBinSize) + 1 #+1 for blank state at the end of each character\n",
    "\n",
    "nStates = (nStates + 1).astype(np.int32) #+1 for blank state at the beginning of the sentence\n",
    "\n",
    "letterStartIdx += 1 #adjust letterStartIdx to account for the blank state at the beginning of the sentence\n",
    "letterStartIdx = letterStartIdx.astype(np.int32)\n",
    "\n",
    "#Next, define the state transitions and emission distributions.\n",
    "#The A matrix defines the state transitions (entry [i,j] is the probability of moving from i->j).\n",
    "#The B matrix defines the emission probabilities (by defining mean firing rates for each state). One row for each state.\n",
    "A_hmm = np.zeros([nStates, nStates])\n",
    "B_hmm = np.zeros([nStates, templates[sentence[0]].shape[1]]) \n",
    "\n",
    "#For each state, store which character it belongs to (stateLabels) and that character's position in the sentence (stateLabelsSeq)\n",
    "stateLabels = np.zeros([nStates,1], dtype='str')\n",
    "stateLabelsSeq = np.zeros([nStates,1])\n",
    "nChars = len(templates)\n",
    "\n",
    "#loop through each character in the sentence and add that character's states to the HMM\n",
    "for x in range(len(sentence)):\n",
    "    \n",
    "    #nBins is the number of HMM states in this character\n",
    "    nBins = np.floor(templates[sentence[x]].shape[0]/hmmBinSize).astype(np.int32)\n",
    "    currentState = letterStartIdx[x]\n",
    "    \n",
    "    #idxInTemplate keeps track of where we are in the character template for each HMM state\n",
    "    idxInTemplate = np.arange(0, hmmBinSize).astype(np.int32)\n",
    "\n",
    "    #loop through each HMM state belonging to the current character\n",
    "    for b in range(nBins):\n",
    "        \n",
    "        #define the mean firing rates for this state\n",
    "        meanRates = np.mean(templates[sentence[x]][idxInTemplate,:], axis=0)\n",
    "        B_hmm[currentState, :] = meanRates\n",
    "        \n",
    "        #define which character and position this state belongs to\n",
    "        stateLabels[currentState] = sentence[x]\n",
    "        stateLabelsSeq[currentState] = x\n",
    "        \n",
    "        #define transition probabilities for this state\n",
    "        A_hmm[currentState, currentState] = stayProb\n",
    "\n",
    "        if b<(nBins-1):\n",
    "            #--prior to last state-- \n",
    "            if b<(nBins-2):\n",
    "                #we can advance one state OR skip two states ahead\n",
    "                A_hmm[currentState, currentState+1] = 1-stayProb-skipProb\n",
    "                A_hmm[currentState, currentState+2] = skipProb\n",
    "            else:\n",
    "                #this is the second to last state, so no skipping two states ahead\n",
    "                A_hmm[currentState, currentState+1] = 1-stayProb\n",
    "        else:\n",
    "            #--last state--\n",
    "            #we can either transition to a blank state at the end of the character or go to the next character\n",
    "            \n",
    "            #special blank state\n",
    "            A_hmm[currentState, currentState+1] = (1-stayProb)*blankProb\n",
    "            A_hmm[currentState+1, currentState+1] = 0.5\n",
    "\n",
    "            stateLabels[currentState+1] = '&' #blank symbol\n",
    "            stateLabelsSeq[currentState+1] = -1\n",
    "\n",
    "            #go to the next letter or end the sentence\n",
    "            if x<(len(sentence)-1):\n",
    "                #transition to next letter\n",
    "                A_hmm[currentState, letterStartIdx[x+1]] = (1-stayProb)*(1-blankProb) #last letter state\n",
    "                A_hmm[currentState+1, letterStartIdx[x+1]] = 0.5 #blank\n",
    "            else:\n",
    "                #end of the sentence\n",
    "                A_hmm[currentState, currentState+1] = (1-stayProb)\n",
    "                A_hmm[currentState+1, currentState+1] = 1.0 #stay in blank permanently (this is the end of the sentence)\n",
    "\n",
    "        currentState += 1\n",
    "        idxInTemplate += hmmBinSize\n",
    "\n",
    "#define the beginning blank state\n",
    "stateLabels[0] = '&'\n",
    "stateLabelsSeq[0] = -1\n",
    "                      \n",
    "A_hmm[0,0] = 0.5\n",
    "A_hmm[0,1] = 0.5\n",
    "\n",
    "#fill in emission probabilities for all blank states\n",
    "letterStates = stateLabelsSeq >= 0\n",
    "blankStates = stateLabelsSeq ==-1\n",
    "blankFiringRates = np.mean(B_hmm[letterStates[:,0],:],axis=0,keepdims=True)\n",
    "                      \n",
    "B_hmm[blankStates[:,0],:] = blankFiringRates\n",
    "                      \n",
    "#define the variance for each emission dimension (assuming multivariate normal distribution with diagonal covariance)\n",
    "diagVariance = np.ones([1 ,B_hmm.shape[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the state transition matrix for the HMM.\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(A_hmm,aspect='auto')\n",
    "plt.ylabel('HMM State')\n",
    "plt.xlabel('HMM State')\n",
    "plt.title('HMM State Transition Probabilities (A Matrix)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(A_hmm[0:40,0:40],aspect='auto')\n",
    "plt.ylabel('HMM State')\n",
    "plt.xlabel('HMM State')\n",
    "plt.title('HMM State Transition Probabilities (Zoomed-In)')\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'stateLabels' vector defines the character that each HMM state belongs to.\n",
    "#We print it here, since it might be helpful to get a feel for how many states they are and how they map to characters.\n",
    "#Note that '&' is a blank state.\n",
    "\n",
    "print('State Character Labels')\n",
    "print(np.transpose(stateLabels).tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the neural data to be processed by the HMM by normalizing, smoothing and binning it.\n",
    "neuralCube = dat['neuralActivityCube'].astype(np.float64)\n",
    "\n",
    "#subtract block-specific means from each trial \n",
    "#we use the means from the single letter data (since the templates also use this)\n",
    "for b in range(dat['blockList'].shape[0]):\n",
    "    trialsFromThisBlock = np.squeeze(dat['sentenceBlockNums']==dat['blockList'][b])\n",
    "    trialsFromThisBlock = np.argwhere(trialsFromThisBlock)\n",
    "        \n",
    "    closestIdx = np.argmin(np.abs(singleLetterDat['blockList'].astype(np.int32) - dat['blockList'][b].astype(np.int32)))\n",
    "    blockMeans = singleLetterDat['meansPerBlock'][closestIdx,:]\n",
    "    \n",
    "    neuralCube[trialsFromThisBlock,:,:] -= blockMeans[np.newaxis,np.newaxis,:]\n",
    "\n",
    "#divide by standard deviation to normalize the units\n",
    "neuralCube = neuralCube / singleLetterDat['stdAcrossAllData'][np.newaxis,:,:]\n",
    "\n",
    "#smooth neural activity (important!)\n",
    "import scipy.ndimage\n",
    "\n",
    "neuralCube = scipy.ndimage.gaussian_filter1d(neuralCube, 4.0, axis=1)  # Use the updated namespace\n",
    "\n",
    "#select the neural activity for the example sentence and bin it \n",
    "obsRaw = neuralCube[sentenceIdx,0:dat['numTimeBinsPerSentence'][sentenceIdx,0],:]\n",
    "\n",
    "nHMMSizedBins = np.floor(obsRaw.shape[0]/hmmBinSize).astype(np.int32)\n",
    "obs = np.zeros([nHMMSizedBins, obsRaw.shape[1]])\n",
    "binIdx = np.arange(0, hmmBinSize).astype(np.int32)\n",
    "for x in range(nHMMSizedBins):\n",
    "    obs[x,:] = np.mean(obsRaw[binIdx,:], axis=0)\n",
    "    binIdx += hmmBinSize\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the neural activity that the HMM will process. \n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.transpose(obs), aspect='auto', clim=[-0.5, 0.5])\n",
    "plt.xlabel('Time Bin (50 ms)')\n",
    "plt.ylabel('Electrode #')\n",
    "plt.title('Neural Activity')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the starting state probabilities for the HMM. The following gives a small chance of starting with a blank, or otherwise\n",
    "#it begins with the first state of the first character.\n",
    "startProb = np.zeros([A_hmm.shape[0]])\n",
    "startProb[0] = blankProb\n",
    "startProb[1] = 1 - startProb[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For labeling long sentences, it can help to enforce that characters occur within a certain \n",
    "#time window defined by their lcoation in the sentence. This prevents pathological solutions \n",
    "#that place a large chunk of characters very close together. We implement this using 'probMask'.\n",
    "#This mask is also used to enforce sentence termination (by allowing only the final states to occur at the last time step).\n",
    "\n",
    "#During inference, 'probMask' is multiplied with the observation probabilities. \n",
    "#If it's zero for a given state and time step, then that state cannot appear at that time step.\n",
    "probMask = np.zeros([obs.shape[0], A_hmm.shape[0]])\n",
    "\n",
    "#Determine how big of a window we give each character, depending on sentence length\n",
    "if len(sentence)==1:\n",
    "    winLen = 1\n",
    "elif len(sentence)<=10:\n",
    "    winLen = 0.50\n",
    "else:\n",
    "    winLen = 0.30\n",
    "\n",
    "#Now fill in probMask\n",
    "for x in range(A_hmm.shape[0]):\n",
    "    #first, get this state's character-level position in the sentence\n",
    "    sl = stateLabelsSeq[x]\n",
    "    \n",
    "    #special logic for blank states (-1)\n",
    "    if sl==-1:\n",
    "        sl = stateLabelsSeq[x-1]\n",
    "        \n",
    "    #compute where this state should occur in normalized time\n",
    "    timeFraction = sl/np.max(stateLabelsSeq)\n",
    "        \n",
    "    #compute the window of allowable time steps that this state can appear\n",
    "    totalSteps = obs.shape[0]\n",
    "    tIdx = np.arange(np.round(timeFraction*totalSteps - winLen*totalSteps),\n",
    "                     np.round(timeFraction*totalSteps + winLen*totalSteps)).astype(np.int32)\n",
    "    tIdx = tIdx[np.logical_and(tIdx>=0, tIdx<totalSteps)]\n",
    "    \n",
    "    probMask[tIdx, x] = 1\n",
    "    \n",
    "#sentence termination constraint (disallows all states at the last time step EXCEPT the final character state or blank)\n",
    "probMask[-1, 0:-2] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the probability mask (yellow region indicates band of accepted states for each time step)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(np.transpose(probMask))\n",
    "plt.xlabel('Time Bin (50 ms)')\n",
    "plt.ylabel('HMM State')\n",
    "plt.title('Time Window Mask')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do inference with the HMM using the forward-backward algorithm which gives, for each time step, the probability of each state\n",
    "#occurring at that time step. \n",
    "#(see Chapter 13 of \"Bishop, Christopher M. Pattern Recognition and Machine Learning. New York: Springer, 2011.\")\n",
    "\n",
    "#number of states in the HMM\n",
    "numStates = A_hmm.shape[0] \n",
    "\n",
    "#number of time bins in the sentence plus one\n",
    "L = obs.shape[0] + 1 \n",
    "\n",
    "#For some sentences, the states don't propagate forward with high enough probability and don't reach the end of the sentence, \n",
    "#causing nans. To mitigate this issue, if we detect a failure, we relax the constraints by increasing the observation variance \n",
    "#and try again.\n",
    "successfulCompletion = False\n",
    "\n",
    "#with np.errstate(invalid='raise',divide='raise',over='raise'):\n",
    "while not successfulCompletion:\n",
    "\n",
    "    #Initialize the forward / backward probability vectors (fs, bs) and scaling factor (s).\n",
    "    #fs = P(Xt | O1, ..., Ot)\n",
    "    #bs ∝ P(Ot+1, ..., OT | Xt)\n",
    "    #fs and bs are combined to compute P(Xt | O1, ... OT) ∝ P(Xt | O1, ..., Ot)P(Ot+1, ..., OT | Xt)\n",
    "\n",
    "    fs = np.zeros([numStates, L])\n",
    "    fs[:, 0] = startProb\n",
    "\n",
    "    bs = np.ones([numStates, L])\n",
    "    logBS = np.ones([numStates, L])\n",
    "\n",
    "    s = np.zeros([L])\n",
    "    s[0] = 1\n",
    "\n",
    "    #forward pass\n",
    "    for count in range(1, L):\n",
    "        #multivariate normal observation probabilities P(Ot | Xt)\n",
    "        squaredDiffTerm = -np.square(B_hmm-obs[count-1,:])/(2*diagVariance)\n",
    "        gaussianEmissionProb = np.exp(np.sum(squaredDiffTerm, axis=1))\n",
    "\n",
    "        #recursive computation of fs\n",
    "        fs[:,count] = gaussianEmissionProb * np.matmul(np.transpose(A_hmm), fs[:,count-1]) * probMask[count-1,:]\n",
    "\n",
    "        #scaling\n",
    "        s[count] =  np.sum(fs[:,count])\n",
    "        fs[:,count] =  fs[:,count]/s[count]   \n",
    "\n",
    "    #backward pass\n",
    "    for count in range(L-2,-1,-1):\n",
    "        #multivariate normal observation probabilities P(Ot+1 | Xt+1)\n",
    "        squaredDiffTerm = -np.square(B_hmm-obs[count,:])/(2*diagVariance)\n",
    "        gaussianEmissionProb = np.exp(np.sum(squaredDiffTerm, axis=1))\n",
    "\n",
    "        #recursive computation of bs\n",
    "        bs[:,count] = np.matmul(A_hmm, bs[:,count+1] * gaussianEmissionProb * probMask[count,:])\n",
    "\n",
    "        #scaling\n",
    "        bs[:,count] = bs[:,count]/s[count+1]\n",
    "\n",
    "    #final probabilities\n",
    "    pSeq = np.sum(np.log(s))\n",
    "    pStates = fs*bs\n",
    "\n",
    "    #get rid of the first column\n",
    "    pStates = pStates[:, 1:]\n",
    "\n",
    "    if not np.any(np.isnan(pStates)):\n",
    "        successfulCompletion = True\n",
    "    else:\n",
    "        #increase the variance a bit and try again\n",
    "        print('Increasing variance to help states propagate to the end of the sentence.')\n",
    "        diagVariance += 0.5\n",
    "\n",
    "print('Forward-backward algorithm completed successfuly.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the log probabilities for each state over time. The path formed through the states shows an orderly progression through \n",
    "#the character sequence (as expected). Branches from the main path indicate alternative labelings.\n",
    "\n",
    "#note that we convert negative infinity values into finite values for plotting (otherwise they appear white)\n",
    "logProb = np.log(pStates[0:300, 0:300])\n",
    "logProb[logProb==-np.inf] = -1000\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(logProb,clim=[-20,0],aspect='auto')\n",
    "plt.title('State Probabilities')\n",
    "plt.xlabel('Time Bin (50 ms)')\n",
    "plt.ylabel('HMM State')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The forward-backward algorithm gives state probabilities but not the most likely single sequence.\n",
    "#We now use the Viterbi algorithm to find the most likely sequence of states, working with log probabilities\n",
    "#to avoid numerical issues.\n",
    "\n",
    "numStates = A_hmm.shape[0]\n",
    "L = obs.shape[0]\n",
    "logTR = np.log(A_hmm)\n",
    "\n",
    "#pTR stores, for each state S and time step T, the previous state in the most likely path ending in S at time T.\n",
    "#This can be used to backtrace the most likely path, beginning at the most likely state on the final time step. \n",
    "pTR = np.zeros([numStates,L])\n",
    "\n",
    "#v stores, for each state, the probability of the most likely path that ends in that state.\n",
    "v = np.log(startProb[:,np.newaxis])\n",
    "\n",
    "#loop through each time step, updating pTR and v one step at a time\n",
    "for count in range(L):\n",
    "    #multivariate normal observation probabilities for this time step P(Ot | Xt)\n",
    "    squaredDiffTerm = -np.square(B_hmm-obs[count,:])/(2*diagVariance)\n",
    "    gaussianEmissionProb = np.sum(squaredDiffTerm, axis=1, keepdims=True)\n",
    "\n",
    "    #recursively update v; for each state, find the best way to get there from the previous time step\n",
    "    #and keep track of it in pTR\n",
    "    tmpV = v + logTR\n",
    "    maxIdx = np.argmax(tmpV, axis=0)\n",
    "    maxVal = np.take_along_axis(tmpV, np.expand_dims(maxIdx, axis=0), axis=0)\n",
    "\n",
    "    v = gaussianEmissionProb + np.transpose(maxVal) + np.log(probMask[count,:,np.newaxis])\n",
    "    pTR[:,count] = maxIdx\n",
    "\n",
    "#decide which of the final states is most probable\n",
    "finalState = np.argmax(v)\n",
    "logP = v[finalState]\n",
    "\n",
    "#Now back trace through pTR to get the most likely state path\n",
    "viterbiStates = np.zeros([L]).astype(np.int32)\n",
    "viterbiStates[-1] = finalState\n",
    "for count in range(L-2,0,-1):\n",
    "    viterbiStates[count] = pTR[viterbiStates[count+1], count+1]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Viterbi sequence of states with a red trace on top of the state probabilities.\n",
    "logProb = np.log(pStates[0:300, 0:300])\n",
    "logProb[logProb==-np.inf] = -1000\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(logProb, clim=[-20,0], aspect='auto')\n",
    "plt.plot(np.arange(0,300), viterbiStates[0:300],'-r',linewidth=2)\n",
    "plt.xlim([0,300])\n",
    "plt.ylim([0,300])\n",
    "\n",
    "plt.xlabel('Time Bin (50 ms)')\n",
    "plt.ylabel('HMM State')\n",
    "plt.title('State Probabilities and Viterbi Path')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get  character start times and character stretch factors from the Viterbi sequence of HMM states.\n",
    "labeledStates = stateLabelsSeq[viterbiStates]\n",
    "\n",
    "letterStarts = np.zeros([len(sentence),1])\n",
    "letterStretches = np.zeros([len(sentence),1])\n",
    "for x in range(len(sentence)):\n",
    "    thisChar = np.argwhere(labeledStates[:,0]==x)\n",
    "    letterStarts[x] = (thisChar[0]+1)*hmmBinSize-1\n",
    "    letterStretches[x] = (len(thisChar)*hmmBinSize)/templates[sentence[x]].shape[0]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this next step, we refine the start times of each character by shifting them around a bit \n",
    "#until they lie on correlation hotspots\n",
    "for c in range(len(sentence)):\n",
    "    #generate a list of potential start times for this character\n",
    "    possibleStart = np.arange(letterStarts[c]-50, letterStarts[c]+55, 5).astype(np.int32)\n",
    "    possibleStart = possibleStart[possibleStart>=0]\n",
    "    \n",
    "    #don't allow start times that are too close to the previous character\n",
    "    if c>0:\n",
    "        possibleStart = possibleStart[possibleStart>=(letterStarts[c-1]+20)]\n",
    "        \n",
    "    possibleStretch = np.linspace(0.4, 1.5, 15)\n",
    "    template = templates[sentence[c]]\n",
    "    \n",
    "    corrHeatmap = np.zeros([len(possibleStretch), len(possibleStart)])\n",
    "    corrHeatmap[:] = -np.inf\n",
    "    \n",
    "    #compute correlation heatmap by correlating the template to the data at each stretch factor and start location\n",
    "    for (stretch, stretchCount) in zip(possibleStretch, range(len(possibleStretch))):\n",
    "        newX = np.linspace(0,1,int(np.round(template.shape[0]*stretch)))\n",
    "        stretchedTemplate = np.zeros([len(newX), template.shape[1]])\n",
    "        for colIdx in range(template.shape[1]):\n",
    "            stretchedTemplate[:,colIdx] = np.interp(newX, np.linspace(0,1,template.shape[0]), template[:,colIdx])\n",
    "            \n",
    "        for (startIdx, startCount) in zip(possibleStart, range(len(possibleStart))):\n",
    "            #don't evaluate possibilities that intersect the previous template\n",
    "            if c>0:\n",
    "                prevTemplateEnd = letterStarts[c-1] + letterStretches[c-1]*templates[sentence[c-1]].shape[0]\n",
    "                if startIdx < (prevTemplateEnd-10):\n",
    "                    continue\n",
    "            \n",
    "            #don't evaluate possibilities that intersect the next template\n",
    "            if c<len(sentence)-1:\n",
    "                thisTemplateEnd = startIdx + stretchedTemplate.shape[0]\n",
    "                if thisTemplateEnd > letterStarts[c+1]+10:\n",
    "                    continue\n",
    "                    \n",
    "            #don't evaluate possibilities that lie outside of the data range\n",
    "            stepIdx = np.arange(startIdx, startIdx + stretchedTemplate.shape[0]).astype(np.int32)\n",
    "            if stepIdx[-1]>=(obsRaw.shape[0]):\n",
    "                continue\n",
    "                \n",
    "            #compute correlation between this template and the data\n",
    "            msDat = obsRaw[stepIdx,:] - np.mean(obsRaw[stepIdx,:], axis=0, keepdims=True)\n",
    "            msST = stretchedTemplate - np.mean(stretchedTemplate, axis=0, keepdims=True)\n",
    "            \n",
    "            normDat = np.sqrt(np.sum(np.square(msDat), axis=0))\n",
    "            normST = np.sqrt(np.sum(np.square(msST), axis=0))\n",
    "                            \n",
    "            corrCoeff = np.sum(msDat * msST, axis=0) / (normDat * normST)\n",
    "            corrHeatmap[stretchCount, startCount] = np.nanmean(corrCoeff)\n",
    "\n",
    "    #select character stretch factors and start times based on the heatmap hotspot\n",
    "    maxIdx = np.argmax(corrHeatmap)\n",
    "    maxIdx = np.unravel_index(maxIdx, corrHeatmap.shape)\n",
    "    \n",
    "    letterStretches[c] = possibleStretch[maxIdx[0]]\n",
    "    letterStarts[c] = possibleStart[maxIdx[1]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we make character heatmaps to validate successful labeling. This is just a qualitative way to visualize what's\n",
    "#going on (i.e. it's a useful sanity check). The heatmaps are made by correlating a character template to the data with\n",
    "#different start times and contraction/dilation of the template. If the HMM labeling process worked well, the \n",
    "#inferred character start times should lie on a hotspot. They often do, but not always.\n",
    "heatmapList = []\n",
    "\n",
    "for x in range(len(sentence)):\n",
    "    possibleStart = np.arange(letterStarts[x]-200, letterStarts[x]+200, 10).astype(np.int32)\n",
    "    possibleStretch = np.linspace(0.4,1.5,15);\n",
    "    thisTemplate = templates[sentence[x]]\n",
    "    \n",
    "    corrHeatmap = np.zeros([len(possibleStretch), len(possibleStart)])\n",
    "\n",
    "    for (stretch, stretchCount) in zip(possibleStretch, range(len(possibleStretch))):\n",
    "        #make new dilated/contracted template\n",
    "        newX = np.linspace(0,1,int(thisTemplate.shape[0]*stretch))\n",
    "        stretchedTemplate = np.zeros([len(newX), thisTemplate.shape[1]])\n",
    "        for colIdx in range(thisTemplate.shape[1]):\n",
    "            stretchedTemplate[:,colIdx] = np.interp(newX, np.linspace(0,1,thisTemplate.shape[0]), thisTemplate[:,colIdx])\n",
    "            \n",
    "        for (startIdx, startCount) in zip(possibleStart, range(len(possibleStart))):\n",
    "            #check if template falls oustide of the data boundaries\n",
    "            if startIdx < 0:\n",
    "                continue\n",
    "            if startIdx + stretchedTemplate.shape[0] >= obsRaw.shape[0]:\n",
    "                continue\n",
    "                \n",
    "            #compute correlation between this template and the data\n",
    "            stepIdx = np.arange(startIdx, startIdx+stretchedTemplate.shape[0]).astype(np.int32)\n",
    "            \n",
    "            msDat = obsRaw[stepIdx,:] - np.mean(obsRaw[stepIdx,:], axis=0, keepdims=True)\n",
    "            msST = stretchedTemplate - np.mean(stretchedTemplate, axis=0, keepdims=True)\n",
    "            \n",
    "            normDat = np.sqrt(np.sum(np.square(msDat), axis=0))\n",
    "            normST = np.sqrt(np.sum(np.square(msST), axis=0))\n",
    "            \n",
    "            corrCoeff = np.sum(msDat * msST, axis=0) / (normDat * normST)\n",
    "            corrHeatmap[stretchCount, startCount] = np.nanmean(corrCoeff)\n",
    "            \n",
    "    heatmapList.append(corrHeatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the heatmaps generated above for the first 16 characters of the sentence. For each heatmap, the x-axis is time (in seconds)\n",
    "#and the y-axis is the 'stretch factor' (linear time dilation/contraction)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plotIdx = np.arange(0, 20).astype(np.int32)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "for p in range(len(plotIdx)):\n",
    "    \n",
    "    plt.subplot(5,4,p+1)\n",
    "    print(f\"letterStarts[{x}]: {letterStarts[x]}\")\n",
    "    print(f\"letterStretches[{x}]: {letterStretches[x]}\")\n",
    "    x = plotIdx[p]\n",
    "    for i, heatmap in enumerate(heatmapList):\n",
    "        print(f\"heatmapList[{i}] shape: {np.shape(heatmap)}\")\n",
    "    plt.imshow(heatmapList[x], aspect='auto', clim=[-0.1, 0.15], \n",
    "           extent=[letterStarts[x][0]/100-2, letterStarts[x][0]/100+2, 1.5, 0.4])\n",
    "    plt.plot(letterStarts[x][0]/100, letterStretches[x][0], 'kX', markersize=12)\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(sentence[x] + ' (' + str(letterStarts[x,0]/100) + ' s)')\n",
    "    \n",
    "plt.gcf().suptitle('Character Labeling Heatmaps',y=1.02)\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we construct the time series 'targets' used to train the RNN. The RNN is trained using supervised learning to \n",
    "#produce the following two outputs: a character probability vector with a one-hot encoding of the current character, \n",
    "#and a binary'new character' signal which briefly goes high at the start of any new character. \n",
    "\n",
    "#Note that we also produce an 'ignoreError' vector which describes, for each time step, whether the cost function should ignore\n",
    "#any errors at that time step. We use this feature to prevent the RNN from being pnealized for errors that occur at the very\n",
    "#start of the trial, before T5 has written any character yet (if the HMM has labeled this as a 'blank' state).\n",
    "\n",
    "\n",
    "# Definiamo i dati di input e i target\n",
    "charProbTarget = np.zeros([obsRaw.shape[0], len(charDef['charList'])])\n",
    "charStartTarget = np.zeros([obsRaw.shape[0], 1])\n",
    "ignoreErrorHere = np.zeros([obsRaw.shape[0], 1])\n",
    "\n",
    "# Creazione dei target di probabilità di carattere e segnale di inizio\n",
    "for x in range(len(sentence)):\n",
    "    # Otteniamo gli indici temporali da riempire\n",
    "    if x < (len(sentence) - 1):\n",
    "        stepIdx = np.arange(letterStarts[x], letterStarts[x + 1]).astype(np.int32)\n",
    "    else:\n",
    "        stepIdx = np.arange(letterStarts[x], obsRaw.shape[0]).astype(np.int32)\n",
    "\n",
    "    # One-hot encoding del carattere corrente\n",
    "    charIdx = np.squeeze(np.argwhere(np.array(charDef['charListAbbr']) == sentence[x]))\n",
    "    charProbTarget[stepIdx, charIdx] = 1\n",
    "\n",
    "    # Segnale di inizio del carattere per i primi 200 ms (21 step)\n",
    "    stepIdx = np.arange(letterStarts[x], letterStarts[x] + 21).astype(np.int32)\n",
    "    charStartTarget[stepIdx] = 1\n",
    "\n",
    "# Riempimento dell'inizio con il primo carattere e ignoriamo gli errori\n",
    "charIdx = np.squeeze(np.argwhere(np.array(charDef['charListAbbr']) == sentence[0]))\n",
    "charProbTarget[0:letterStarts[0, 0].astype(np.int32), charIdx] = 1\n",
    "ignoreErrorHere[0:letterStarts[0, 0].astype(np.int32)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the RNN targets for this sentence\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(np.transpose(charProbTarget), aspect='auto')\n",
    "plt.xlabel('Time Step (10 ms)')\n",
    "plt.ylabel('Character')\n",
    "plt.title('One-Hot Character Probabilities')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(charStartTarget)\n",
    "plt.xlabel('Time Step (10 ms)')\n",
    "plt.ylabel('New Character Signal')\n",
    "plt.title('New Character Signal')\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
