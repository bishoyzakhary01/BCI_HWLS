{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#suppress all tensorflow warnings (largely related to compatability with v2)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from charSeqLSTM import charSeqLSTM, getDefaultLSTMArgs\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = os.path.expanduser('~') + '/handwritingBCIData/'\n",
    "\n",
    "#evaluate the LSTM on these datasets\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "#use this train/test partition\n",
    "cvPart = 'HeldOutTrials'\n",
    "\n",
    "#point this towards the specific LSTM we want to evaluate\n",
    "LSTMOutputDir = cvPart\n",
    "\n",
    "#this prevents tensorflow from taking over more than one gpu on a multi-gpu machine\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "#this is where we're going to save the LSTM outputs\n",
    "inferenceSaveDir = rootDir+'LSTMTrainingSteps/Step5_LSTMInference/' + LSTMOutputDir\n",
    "\n",
    "if not os.path.isdir(rootDir + 'LSTMTrainingSteps/Step5_LSTMInference'):\n",
    "    os.mkdir(rootDir + 'LSTMTrainingSteps/Step5_LSTMInference')\n",
    "    \n",
    "if not os.path.isdir(inferenceSaveDir):\n",
    "    os.mkdir(inferenceSaveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Processing dataset t5.2019.05.08\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m args[\u001b[39m'\u001b[39m\u001b[39minferenceInputLayer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m x\n\u001b[1;32m     28\u001b[0m \u001b[39m#instantiate the LSTM model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m LSTMModel \u001b[39m=\u001b[39m charSeqLSTM(args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m     31\u001b[0m \u001b[39m#evaluate the LSTM on the held-out data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m outputs \u001b[39m=\u001b[39m LSTMModel\u001b[39m.\u001b[39minference()\n",
      "File \u001b[0;32m~/Documents/GitHub/BCIHWLSTM/charSeqLSTM.py:60\u001b[0m, in \u001b[0;36mcharSeqLSTM.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m#load data, labels, train/test partitions & synthetic .tfrecord files for all days\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m neuralCube_all, targets_all, errWeights_all, numBinsPerTrial_all, cvIdx_all, recordFileSet_all \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loadAllDatasets()\n\u001b[1;32m     62\u001b[0m \u001b[39m#define the input & output dimensions of the LSTM\u001b[39;00m\n\u001b[1;32m     63\u001b[0m nOutputs \u001b[39m=\u001b[39m targets_all[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 5)"
     ]
    }
   ],
   "source": [
    "#Configures the LSTM for inference mode.\n",
    "args = getDefaultLSTMArgs()\n",
    "\n",
    "args['outputDir'] = rootDir+'LSTMTrainingSteps/Step4_LSTMTraining/'+LSTMOutputDir\n",
    "args['loadDir'] = args['outputDir']\n",
    "args['mode'] = 'infer'\n",
    "args['timeSteps'] = 7500 #Need to specify enough time steps so that the longest sentence fits in the minibatch\n",
    "args['batchSize'] = 2 #Process just two sentences at a time, to make sure we have enough memory\n",
    "args['synthBatchSize'] = 0 #turn off synthetic data here, we are only using real data\n",
    "\n",
    "#Proceeds one dataset at a time. Currently the code is setup to only process a single dataset at inference time,\n",
    "#so we have to rebuild the graph for each dataset.\n",
    "for x in range(len(dataDirs)):\n",
    "    #configure the LSTM to process this particular dataset\n",
    "    print(' ')\n",
    "    print('Processing dataset ' + dataDirs[x])\n",
    "    \n",
    "    args['sentencesFile_0'] = rootDir+'Datasets/'+dataDirs[x]+'/sentences.mat'\n",
    "    args['singleLettersFile_0'] = rootDir+'Datasets/'+dataDirs[x]+'/singleLetters.mat'\n",
    "    args['labelsFile_0'] = rootDir+'LSTMTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDirs[x]+'_timeSeriesLabels.mat'\n",
    "    args['syntheticDatasetDir_0'] = rootDir+'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart+'/'+dataDirs[x]+'_syntheticSentences/'\n",
    "    args['cvPartitionFile_0'] = rootDir+'LSTMTrainingSteps/trainTestPartitions_'+cvPart+'.mat'\n",
    "    args['sessionName_0'] = dataDirs[x]\n",
    "\n",
    "    args['inferenceOutputFileName'] = inferenceSaveDir + '/' + dataDirs[x] + '_inferenceOutputs.mat'\n",
    "    args['inferenceInputLayer'] = x\n",
    "    \n",
    "    #instantiate the LSTM model\n",
    "    LSTMModel = charSeqLSTM(args=args)\n",
    "\n",
    "    #evaluate the LSTM on the held-out data\n",
    "    outputs = LSTMModel.inference()\n",
    "    \n",
    "    #reset the graph to make space for the next dataset\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell loads the outputs produced above and computes character error counts and word error counts.\n",
    "from characterDefinitions import getHandwritingCharacterDefinitions\n",
    "from LSTMEval import evaluateLSTMOutput, LSTMOutputToKaldiMatrices\n",
    "import warnings\n",
    "\n",
    "#this stops scipy.io.savemat from throwing a warning about empty entries\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "charDef = getHandwritingCharacterDefinitions()\n",
    "allErrCounts = []\n",
    "\n",
    "for x in range(len(dataDirs)):\n",
    "    print('-- ' + dataDirs[x] + ' --')\n",
    "    \n",
    "    #Load up the outputs, which are frame-by-frame probabilities. \n",
    "    outputs = scipy.io.loadmat(inferenceSaveDir + '/' + dataDirs[x] + '_inferenceOutputs.mat')\n",
    "    sentenceDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDirs[x]+'/sentences.mat')\n",
    "    \n",
    "    #Convert the outputs into character sequences (with simple thresholding) & get word/character error counts.\n",
    "    errCounts, decSentences = evaluateLSTMOutput(outputs['outputs'], \n",
    "                                        sentenceDat['numTimeBinsPerSentence']/2 + 50, \n",
    "                                        sentenceDat['sentencePrompt'], \n",
    "                                        charDef, \n",
    "                                        charStartThresh=0.3, \n",
    "                                        charStartDelay=15)\n",
    "    \n",
    "    #save decoded sentences, character error rates and word error rates for later summarization\n",
    "    saveDict = {}\n",
    "    saveDict['decSentences'] = decSentences\n",
    "    saveDict['trueSentences'] = sentenceDat['sentencePrompt']\n",
    "    saveDict.update(errCounts)\n",
    "    \n",
    "    scipy.io.savemat(inferenceSaveDir + '/' + dataDirs[x] + '_errCounts.mat', saveDict)\n",
    "    \n",
    "    #print results for the validation sentences\n",
    "    cvPartFile = scipy.io.loadmat(rootDir+'LSTMTrainingSteps/trainTestPartitions_'+cvPart+'.mat')\n",
    "    valIdx = cvPartFile[dataDirs[x]+'_test']\n",
    "    \n",
    "    if len(valIdx)==0:\n",
    "        print('No validation sentences for this session.')\n",
    "        print('  ')\n",
    "        continue\n",
    "            \n",
    "    valAcc = 100*(1 - np.sum(errCounts['charErrors'][valIdx]) / np.sum(errCounts['charCounts'][valIdx]))\n",
    "\n",
    "    print('Character error rate for this session: %1.2f%%' % float(100-valAcc))\n",
    "    print('Below is the decoder output for all validation sentences in this session:')\n",
    "    print(' ')\n",
    "    \n",
    "    for v in np.squeeze(valIdx):\n",
    "        trueText = sentenceDat['sentencePrompt'][v,0][0]\n",
    "        trueText = trueText.replace('>',' ')\n",
    "        trueText = trueText.replace('~','.')\n",
    "        trueText = trueText.replace('#','')\n",
    "        \n",
    "        print('#' + str(v) + ':')\n",
    "        print('True:    ' + trueText)\n",
    "        print('Decoded: ' + decSentences[v])\n",
    "        print(' ')\n",
    "   \n",
    "    #put together all the error counts from all sessions so we can compute overall error rates below\n",
    "    allErrCounts.append(np.stack([errCounts['charCounts'][valIdx],\n",
    "                             errCounts['charErrors'][valIdx],\n",
    "                             errCounts['wordCounts'][valIdx],\n",
    "                             errCounts['wordErrors'][valIdx]],axis=0).T)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize character error rate and word error rate across all sessions.\n",
    "concatErrCounts = np.squeeze(np.concatenate(allErrCounts, axis=0))\n",
    "cer = 100*(np.sum(concatErrCounts[:,1]) / np.sum(concatErrCounts[:,0]))\n",
    "wer = 100*(np.sum(concatErrCounts[:,3]) / np.sum(concatErrCounts[:,2]))\n",
    "\n",
    "print('Character error rate: %1.2f%%' % float(cer))\n",
    "print('Word error rate: %1.2f%%' % float(wer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
